{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to variantannotation: package for the aggregation of genomic variant data \n",
    "\n",
    "#### Author: C. Mazzaferro & Kathleen Fisch\n",
    "#### Email: cmazzafe@ucsd.edu\n",
    "#### Date: January 2017\n",
    " \n",
    "## Outline of Notebook\n",
    "<a id = \"toc\"></a>\n",
    "1. <a href = \"#background\">Background</a>\n",
    "2. <a href = \"#setup\">Set Up File and Libraries</a>\n",
    "3. <a href = \"#ANNOVAR\">Run Annovar</a>\n",
    "4. <a href = \"#export1\">Export Data To MongoDB: Method 1</a>\n",
    "5. <a href = \"#export2\">Export Data To MongoDB: Method 2</a>\n",
    "6. <a href = \"#filter\">Variant Filtering</a>\n",
    "    * <a href = \"#tumorvars\">Rare Tumor Variant Filter</a>\n",
    "    * <a href = \"#diseasevars\">Rare Diesease Variant Filter</a>\n",
    "    * <a href = \"#caddvars\">CADD PHRED High Impact Variants</a>\n",
    "    * <a href = \"#own\">Create Your Own Filter</a>\n",
    "7. <a href = \"#export\">Export CSV and VCF files</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"background\"></a>\n",
    "## Background\n",
    "\n",
    "This notebook will walk you through the steps of how variants coming from a VCF can be annotated efficiently and thoroughly using the package VAPr. In particular, the package is aimed at providing a way of retrieving variant information using [ANNOVAR](http://annovar.openbioinformatics.org/en/latest/) and [myvariant.info](myvariant.info) and consolidating it in conveninent formats. It is well-suited for bioinformaticians interested in aggregating variant information into a single database for ease of use and to provide higher analysis capabities. \n",
    "\n",
    "The aggregation is performed specifically by structuring the data in lists of python dictionaries, with each variant being described by a multi-level dictionary. The choice was made due to the inconsistencies that exist between data availability, and the necessity to store their information in a flexible manner. Further, this specific format permits its parsing to a MongoDb instance (dictionaries are the python representation of JSON objects), which enables the user to efficiently store, query and filter such data. \n",
    "\n",
    "Finally, the package also has the added functionality to create csv and vcf files from MongoDB. The class Filters allows the user to rapidly query data that meets certain criteria as a list of documents, and the class FileWriter can transform such list into more widely accepted formats such as vcf and csv files. It should be noted that here, the main differential the package offers is the ability to write these files preserving all the annotation data. In the vcf files, for instance, outputs will have a 'Otherinfo' column where all the data coming from ANNOVAR and myvariant.info is condensed (while still preserving its structure). For vcf files, outputs will have around ~120-200 columns, depending on the amount of variant data that can be retrieved from myvvariant.info. \n",
    "\n",
    "Having the data stored in a database offers a variety of benefits. In particular, it enables the user to set customized queries and rapidly iterate over a specific procedure and get maximum reproducibility. It also enables the storage of data coming from different sources, and its rapid access. \n",
    "\n",
    "**Notes on required software**\n",
    "\n",
    "the following libraries will be installed upon installing VAPr:\n",
    "- myvariant\n",
    "- pymongo\n",
    "- pyvcf\n",
    "\n",
    "Other libraries that are needed, but should natively e installed on most OS: \n",
    "\n",
    "- Pandas\n",
    "- Numpy\n",
    "\n",
    "Further, a MongoDB database must be set up. Refer to the documentation page for more information. \n",
    "Similarly, ANNOVAR must be downloaded, alongside with its supporting databases (also listed on the documentation page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image, display, HTML\n",
    "Image(filename=os.path.dirname(os.path.realpath('__file__')) + '/simpler.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Import libraries\n",
    "\n",
    "`VAPr.base` is your interface with the annotation and storing tools offered in the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VAPr import base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ANNOVAR\"></a>\n",
    "## Create an Annotation Project\n",
    "\n",
    "The quickest way to get everything up and running is by creating a project where you specify the required paths. To start off, you will need to have your `vcf` files in a specific directory, and create a new, empty directory to which your annotated files will be written to. \n",
    "\n",
    "It is also required that you specify the location to which you dowloaded annovar. The folder where annovar lives looks like this:\n",
    "\n",
    "    ... /annovar/\n",
    "             annotate_variation.pl\n",
    "             coding_change.pl\n",
    "             convert2annovar.pl\n",
    "             example/    \n",
    "             humandb/\n",
    "             retrieve_seq_from_fasta.pl\n",
    "             table_annovar.pl\n",
    "             variants_reduction.pl    \n",
    "\n",
    "It currently supports the three different genome builds, `hg19`, `hg18`, `hg38`\n",
    "\n",
    "**NOTE**: we can't provide the ANNOVAR package as it requires registration to be downloaded. It is, however, freely available [here](http://annovar.openbioinformatics.org/en/latest/user-guide/download/). Download, unzip and place it in whatever directory you'd like. Make sure you have enough space on disk (~15 GB for the datasets used for annotation).\n",
    "### In case you'd want to run the annotations without having to worry about annovar, you can skip to <a href = \"#export2\">Export Data To MongoDB: Method 2</a>. Note that it will result in an easier process, but less data will be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Directory of input files to be annotated\n",
    "IN_PATH = \"/Volumes/Carlo_HD1/CCBB/VAPr_files/vcf_benchmark/\"\n",
    "\n",
    "# Output file directory\n",
    "OUT_PATH = \"/Volumes/Carlo_HD1/CCBB/VAPr_files/csv_benchmark/\"\n",
    "\n",
    "# Location of your annovar dowload. The folder should contain the following files/directories:\n",
    "ANNOVAR_PATH = '/Volumes/Carlo_HD1/CCBB/annovar/'  \n",
    "\n",
    "# Databse and Collection names (optional)\n",
    "proj_data = {'db_name': 'VariantDBBenchmarkWES',\n",
    "            'project_name': 'collect'}\n",
    "\n",
    "\n",
    "Project = base.AnnotationProject(IN_PATH, \n",
    "                                 OUT_PATH, \n",
    "                                 ANNOVAR_PATH, \n",
    "                                 proj_data,  \n",
    "                                 build_ver='hg19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Databses\n",
    "\n",
    "Download annotation databases from Annovar and other repos. Files will be stored at .../annovar/humandb.  You may open that directory to check the progress: make sure all files are unzipped (VAPr does that automatically) before proceeding. Usually the database hg19_1000g2015aug.zip takes some time to be fully downloaded and unzipped (around 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Project.download_dbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check dowload status\n",
    "This command will open the download directory so that you can check if all files are there and unzipped. Make sure all files are in .txt and .idx format before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "subprocess.Popen(['open', os.path.join(ANNOVAR_PATH, 'humandb')], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Annovar \n",
    "\n",
    "The next command will run the annotations on each file and store them in a directory specified by the file's sample name. Don't worry, VAPr figures out automatically the sampele name, and if there are more than one sample in a single file. The csv files can then be processed and integrated with the data coming from myvariant.info. \n",
    "This command may take a some time to run (5-30 minutes for each file depending on file size). Feel free to open the OUT_PATH directory to see the annotation files being created on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Project.run_annovar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"export1\"></a>\n",
    "## Export Annovar, MyVariant data to MongoDB\n",
    "\n",
    "With this method, data from annovar (as a txt file) will be obtained 1000 lines at a time, one file at a time.\n",
    "\n",
    "As soon as you run the scripts from VAPr, variant data will be retrieved from myvariant.info and the data will automatically be integrated and stored to MongoDB. Database and collection name should be specified, and there must be a running MongoDB connection. The script will set up a client to communicate between python (through pymongo) and the the database.\n",
    "\n",
    "In general, the shell command:\n",
    "\n",
    "`mongod --dbpath ../data/db`  \n",
    "\n",
    "Where data/db is the designated location where the data will be stored, will initiate MongoDB. After this, the script should store data to the directory automatically.\n",
    "For pymongo, and more information on how to set up a Mongo Database: https://docs.mongodb.com/getting-started/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%prun Project.annotate_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code profiling\n",
    "`%prun` serves to  profile the code's performance. The output tells you the runtime, and number of function calls. Feel free to remove it from the cell.\n",
    "\n",
    "### 210215396 function calls  in 905.227 seconds  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Up Annotation With Parallel Processing\n",
    "\n",
    "Alternatively, you may want to store a number of files to MongoDB, which may take a long time. This can be sped up by running multiple annotations at the same time. The syntax is pretty much the same. You just need to specify the number of cores to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun Project.parallel_annotation_and_saving(n_processes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code profiling\n",
    "\n",
    "### 95889 function calls in 238.308 seconds. 4x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"export2\"></a>\n",
    "## Export MyVariant data to MongoDB\n",
    "\n",
    "This will export variant data from a vcf file to mongo, annotating it with MyVariant.info solely. It does not require annovar, and may result in some empty variants. \n",
    "\n",
    "- Easier to run, doesn't require annovar.\n",
    "- Will however be incomplete (some variants will have no information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1041: UserWarning: Duplicate key in file \"/Users/carlomazzaferro/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Csv output dir /Volumes/Carlo_HD1/CCBB/VAPr_files/csv_benchmark/sample_BC001 for sample exists, moving files there\n",
      "INFO:root:Vcf input dir /Volumes/Carlo_HD1/CCBB/VAPr_files/vcf_benchmark/sample_BC001 for sample exists, moving files there\n",
      "(True, '/Volumes/Carlo_HD1/CCBB/VAPr_files/vcf_benchmark/sample_BC001/BC001.final.vcf')\n"
     ]
    }
   ],
   "source": [
    "from VAPr import base\n",
    "\n",
    "# Directory of input files to be annotated\n",
    "IN_PATH = \"/Volumes/Carlo_HD1/CCBB/VAPr_files/vcf_benchmark/\"\n",
    "\n",
    "# Output file directory\n",
    "OUT_PATH = \"/Volumes/Carlo_HD1/CCBB/VAPr_files/csv_benchmark/\"\n",
    "\n",
    "# Location of your annovar dowload. The folder should contain the following files/directories:\n",
    "ANNOVAR_PATH = '/Volumes/Carlo_HD1/CCBB/annovar/'  \n",
    "\n",
    "# Databse and Collection names (optional)\n",
    "proj_data = {'db_name': 'VariantDBBenchmarkWES',\n",
    "            'project_name': 'myvariant_only_collection'}\n",
    "\n",
    "\n",
    "Project = base.AnnotationProject(IN_PATH, \n",
    "                                 OUT_PATH, \n",
    "                                 ANNOVAR_PATH, \n",
    "                                 proj_data,  \n",
    "                                 build_ver='hg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 26/52 [04:15<04:33, 10.53s/it]"
     ]
    }
   ],
   "source": [
    "Project.quick_annotate_and_save(n_processes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output files by filtering\n",
    "Create 4 output files: annotated vcf, annotated csv, filtered vcf, filtered csv. \n",
    "All files will contain information from myvariant and ANNOVAR; the filtered ones will be much smaller in size.\n",
    "### See \"Notes on Variant Filtering & Output Files\" for more info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply filter(s).\n",
    "\n",
    "from VAPr import MongoDB_querying, file_writer\n",
    "importlib.reload(MongoDB_querying)\n",
    "\n",
    "\n",
    "#Define names/paths\n",
    "filepath = '/Volumes/Carlo_HD1/'\n",
    "\n",
    "#The input gzipped vcf file is required to create a filtered one.\n",
    "in_vcf_file = filepath + \"/normal_targeted_seq.vcf.gz\"\n",
    "\n",
    "out_unfiltered_vcf_file = filepath + \"/_unfilterd_vcf_annotated.vcf\"\n",
    "out_unfiltered_csv_file = filepath + \"/_unfiltered_csv_annotated.csv\"\n",
    "\n",
    "rare_cancer_variants_csv = filepath + \"/_rare_cancer_vars.csv\"\n",
    "rare_cancer_variants_vcf = filepath + \"/_rare_cancer_vars.vcf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rare Cancer Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VAPr import MongoDB_querying, file_writer\n",
    "importlib.reload(MongoDB_querying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)  #Filter Object\n",
    "\n",
    "#Three different filters. Let's use the first one.\n",
    "rare_cancer_variants = filter_collection.rare_cancer_variant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create writer object that pulls data from MongoDB\n",
    "my_writer = file_writer.FileWriter(db_name, collection_name)\n",
    "\n",
    "#Write collection to csv and vcf\n",
    "my_writer.generate_unfiltered_annotated_csv(out_unfiltered_csv_file)\n",
    "my_writer.generate_unfiltered_annotated_vcf(in_vcf_file, out_unfiltered_vcf_file)\n",
    "\n",
    "#cancer variants filtered files\n",
    "my_writer.generate_annotated_csv(rare_cancer_variants, rare_cancer_variants_csv)\n",
    "my_writer.generate_annotated_vcf(rare_cancer_variants, in_vcf_file, rare_cancer_variants_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other methods and capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtain list of variant ID's from a vcf file\n",
    "open_file = myvariant_parsing_utils.VariantParsing()\n",
    "variant_list = open_file.get_variants_from_vcf(vcf_file)\n",
    "\n",
    "#Or, if file is too large to be held in memory, iterate over it. \n",
    "variant_list = open_file.get_variants_from_vcf_chunk(vcf_file, 10000, 0)\n",
    "\n",
    "#Since data is a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"filter\"></a>\n",
    "## Notes on Variant Filtering & Output Files\n",
    "\n",
    "Here we implement three different filters that allow for the retrieval of specific variants. The filters are implemented as MongoDB queries, and are designed to provie the user with a set of relevant variants. In case the user would like to define its own querying, a template is provided. \n",
    "The output of the queries is a list of dictionaries (JSON documents), where each dictionary contains data reltive to one variant. \n",
    "\n",
    "Further, the package allows the user to parse these variants into an annotated csv or vcf file. \n",
    "If needed, annotated, unfiltered vcf and csv files can also be created. They will have the same length (number of variants) as the original files, but will contain much more complete annotation data coming from myvariant.info and ANNOVAR databases. \n",
    "\n",
    "To create a csv file, just the filtered output is needed. To create an annotated vcf file, a tab indexed file (.tbi) file is needed (see comments in  section Create unfiltered annotated vcf and csv files at the end of this page). This can be created using tabix.  \n",
    "\n",
    "First, the file needs to be compressed:\n",
    "\n",
    "From the command line, running:\n",
    "\n",
    "`bgzip -c input_file.vcf > input_file.vcf.gz`\n",
    "\n",
    "returns `input_vcf_file.vcf.gz`\n",
    "\n",
    "and running \n",
    "\n",
    "`tabix input_vcf_file.vcf.gz`\n",
    "\n",
    "will return: `input_vcf_file.vcf.gz.tbi`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"tumorvars\"></a>\n",
    "## Filter #1: specifying cancer-specifc rare variants\n",
    "\n",
    " - filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    " - filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    " - filter 3: cosmic70 information is present\n",
    " - filter 4: Func_knownGene is exonic, splicing, or both\n",
    " - filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    " - filter 6: Read Depth (DP) > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '/data/ccbb_internal/interns/Carlo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create output files (if needed): specify name of files and path \n",
    "rare_cancer_variants_csv = filepath + \"/tumor_rna_rare_cancer_vars_csv.csv\"\n",
    "rare_cancer_variants_vcf = filepath + \"/tumor_rna_rare_cancer_vars_vcf.vcf\"\n",
    "input_vcf_compressed = filepath + '/test_vcf/Tumor_RNAseq_variants.vcf.gz'\n",
    "\n",
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_cancer_variants = filter_collection.rare_cancer_variant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rarevars\"></a>\n",
    "## Filter #2: specifying rare disease-specifc (rare) variants\n",
    "\n",
    "- filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    "- filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    "- filter 3: cosmic70 information is present\n",
    "- filter 4: Func_knownGene is exonic, splicing, or both\n",
    "- filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    "- filter 6: Read Depth (DP) > 10\n",
    "- filter 7: Clinvar data is present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_disease_variants = filter_collection.rare_disease_variant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero variants found. Writing a csv output won't make much sense. You can still customize the filters the way you'd like, as you can see on 'Create your own filter'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"caddvars\"></a>\n",
    "## Filter #3: specifying rare disease-specifc (rare) variants with high impact\n",
    "- filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    "- filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    "- filter 3: cosmic70 information is present\n",
    "- filter 4: Func_knownGene is exonic, splicing, or both\n",
    "- filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    "- filter 6: Read Depth (DP) > 10\n",
    "- filter 7: Clinvar data is present \n",
    "- filter 8: cadd.phred > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_high_impact_variants = filter_collection.rare_high_impact_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"own\"></a>\n",
    "## Create your own filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you have a MongoDB instance running, filtering can be perfomed trough pymongo as shown by the code below. If a list is intended to be created from it, simply add: `filter2 = list(filter2)`\n",
    "\n",
    "If you'd like to customize your filters, a good idea would be to look at the available fields to be filtered. Looking at the myvariant.info [documentation](http://docs.myvariant.info/en/latest/doc/data.html), you can see what are all the fields avaialble and can be used for filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.My_Variant_Database\n",
    "collection = db.ANNOVAR_MyVariant_chunks\n",
    "\n",
    "filtered = collection.find({\"$and\": [\n",
    "                                   {\"$or\": [{\"esp6500siv2_all\": {\"$lt\": 0.1}}, {\"esp6500siv2_all\": {\"$exists\": False}}]},\n",
    "                                   {\"$or\": [{\"func_knowngene\": \"exonic\"}, {\"func_knowngene\": \"splicing\"}]},\n",
    "                                   {\"genotype.filter_passing_reads_count\": {\"$gte\": 1}},\n",
    "                                   {\"cosmic70\": {\"$exists\": True}},\n",
    "                                   {\"1000g2015aug_all\": {\"$exists\": True}}\n",
    "                         ]})\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
